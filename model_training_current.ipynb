{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading dataset from dataset [https://drive.google.com/drive/folders/1-vrY2Ln9XaTvRi3LqhEIsWBvcMdHNWp9?usp=sharing]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TXvI3voCYo0",
        "outputId": "b08035cc-22a7-4e83-eb63-0662e281a889"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "os.getcwd()\n",
        "os.chdir('gdrive/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.getcwd()\n",
        "os.chdir('gdrive/MyDrive')\n",
        "os.getcwd()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAob49FR52EG"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "from itertools import chain\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "from scipy.io import arff\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def splitting_data(\n",
        "    df, df_target=None, take_time_stamps=124, overlap=62, zero_padding=True, predict_n_future_timesteps=-1, **kwargs\n",
        "):\n",
        "    def windowed_view_adj(\n",
        "        arr, window=take_time_stamps, overlap=overlap, zero_padding=zero_padding, \n",
        "        predict_n_future_timesteps=predict_n_future_timesteps, **kwargs\n",
        "    ):\n",
        "        windows = windowed_view(arr, window, overlap, predict_n_future_timesteps, **kwargs)\n",
        "        if zero_padding:\n",
        "            re = add_zero_padding(arr, window, overlap)\n",
        "            return np.append(windows, re, axis=0)\n",
        "        return windows\n",
        "\n",
        "    def calculate_number_of_created_samples(\n",
        "        arr, window=take_time_stamps, overlap=overlap, zero_padding=zero_padding,\n",
        "        predict_n_future_timesteps=predict_n_future_timesteps, **kwargs\n",
        "    ):\n",
        "        window_step = window - overlap\n",
        "        new_shape = ((arr.shape[-1] - overlap) // window_step, window)\n",
        "        return new_shape[0] + 1 if zero_padding else new_shape[0]\n",
        "\n",
        "    vals = df.values\n",
        "    vals_shape = vals.shape\n",
        "    if vals_shape[1] >= take_time_stamps:\n",
        "        if df_target is None:\n",
        "            data = list(map(windowed_view_adj, vals))\n",
        "            return data, None\n",
        "        else:\n",
        "            targ_data = df_target.values\n",
        "            values_1 = vals if predict_n_future_timesteps in (-1, 0) else vals[:, : -predict_n_future_timesteps]\n",
        "            temp_re = [\n",
        "                (\n",
        "                    [\n",
        "                        windowed_view_adj(l),\n",
        "                        np.array(list(d) * calculate_number_of_created_samples(l)),\n",
        "                    ]\n",
        "                )\n",
        "                for l, d in zip(values_1, targ_data)\n",
        "            ]\n",
        "            data, data_target = zip(*temp_re)\n",
        "            data = np.array(data)\n",
        "            dat_shape = data.shape\n",
        "            data = data.reshape(dat_shape[0] * dat_shape[1], dat_shape[-1])\n",
        "            data_target = list(chain(*data_target))\n",
        "            assert data.shape[0] == len(\n",
        "                data_target\n",
        "            ), \"Target and data rows are different size!\"\n",
        "            if predict_n_future_timesteps in (-1, 0):\n",
        "                return data, data_target\n",
        "            # additional processing\n",
        "            values = vals[:, predict_n_future_timesteps:]\n",
        "            # taking last element which will not be full:\n",
        "            data_y = np.array(list(map(windowed_view_adj, values)))\n",
        "            data_y = data_y.reshape(data.shape)\n",
        "            a_1 = data_y[0][:-predict_n_future_timesteps]\n",
        "            b_1 = data[0][predict_n_future_timesteps:]\n",
        "            a_2 = data_y[-1][:-predict_n_future_timesteps]\n",
        "            b_2 = data[-1][predict_n_future_timesteps:]\n",
        "            assert np.all(a_1 == b_1), \"Y Data is not equal!\"\n",
        "            assert np.all(a_2 == b_2), \"Y Data is not equal!\"\n",
        "            return data, data_y, data_target\n",
        "\n",
        "    else:\n",
        "        print(\"Not enough samples\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def windowed_view(arr, window, overlap):\n",
        "    arr = np.asarray(arr)\n",
        "    window_step = window - overlap\n",
        "    new_shape = arr.shape[:-1] + ((arr.shape[-1] - overlap) // window_step, window)\n",
        "    new_strides = arr.strides[:-1] + (window_step * arr.strides[-1],) + arr.strides[-1:]\n",
        "    return as_strided(arr, shape=new_shape, strides=new_strides)\n",
        "\n",
        "\n",
        "def add_zero_padding(arr, window, overlap):\n",
        "    # need_zeros = len(arr)\n",
        "    array_len = len(arr)\n",
        "    window_step = window - overlap\n",
        "    number_of_els = (arr.shape[-1] - overlap) // window_step\n",
        "    take_ind = number_of_els * window_step\n",
        "    number_of_left_elements = array_len - take_ind\n",
        "    padded_arr = np.array(\n",
        "        list(arr[take_ind:]) + (window - number_of_left_elements) * [0]\n",
        "    ).reshape(1, window)\n",
        "    assert padded_arr.shape == (\n",
        "        1,\n",
        "        window,\n",
        "    ), f\"Wrong dimensions after zero padding, expected (1, {window}), got {padded_arr.shape}\"\n",
        "    return padded_arr\n",
        "\n",
        "\n",
        "def load_preprocessed_datasets_and_processe(\n",
        "    main_data_folder,\n",
        "    exclude_dataset_for_testing,\n",
        "    save_result_folder=None,\n",
        "    windows_size=128,\n",
        "    overlap=64,\n",
        "    zero_padding=False,\n",
        "    predict_n_future_timesteps=-1,\n",
        "):\n",
        "    train_data_dict = {}\n",
        "    test_data_dict = {}\n",
        "    train_target_dict = {}\n",
        "    test_target_dict = {}\n",
        "    data_folders = os.listdir(main_data_folder)\n",
        "    exceptions = {}\n",
        "    predict_future_param_is_set = False if predict_n_future_timesteps in (-1, 0) else True\n",
        "    print(f\"Total datasets {len(data_folders)}\")\n",
        "    for f in data_folders:\n",
        "        try:\n",
        "            test_df = pd.read_csv(f\"{main_data_folder}/{f}/test.csv\")\n",
        "            target_test_df = pd.read_csv(f\"{main_data_folder}/{f}/test_target.csv\")\n",
        "            test_shape = test_df.shape\n",
        "            if test_shape[1] < windows_size or (predict_future_param_is_set and test_shape[1] < windows_size + predict_n_future_timesteps):\n",
        "                exceptions[\n",
        "                    f\n",
        "                ] = f\"Not enough samples in row, found {test_shape[1]}, expected (window size) {windows_size}\"\n",
        "                continue\n",
        "            if f == exclude_dataset_for_testing:\n",
        "                splitted_train, splitted_train_target = pd.DataFrame(), pd.DataFrame()\n",
        "                continue\n",
        "\n",
        "        \n",
        "            train_df = pd.read_csv(f\"{main_data_folder}/{f}/train.csv\")\n",
        "            test_df = pd.read_csv(f\"{main_data_folder}/{f}/test.csv\")\n",
        "            target_train_df = pd.read_csv(\n",
        "                f\"{main_data_folder}/{f}/train_target.csv\"\n",
        "            )\n",
        "            additional_data_params = {'predict_n_future_timesteps': predict_n_future_timesteps} if predict_future_param_is_set else {}\n",
        "            \n",
        "            #splitted_train, splitted_train_target \n",
        "            splitted_train_data = splitting_data(\n",
        "                train_df,\n",
        "                target_train_df,\n",
        "                take_time_stamps=windows_size,\n",
        "                overlap=overlap,\n",
        "                zero_padding=zero_padding,\n",
        "                **additional_data_params\n",
        "                #predict_n_future_timesteps=predict_n_future_timesteps\n",
        "            )\n",
        "            #splitted_test, splitted_test_target \n",
        "            splitted_test_data = splitting_data(\n",
        "                test_df,\n",
        "                target_test_df,\n",
        "                take_time_stamps=windows_size,\n",
        "                overlap=overlap,\n",
        "                zero_padding=zero_padding,\n",
        "                **additional_data_params\n",
        "                #predict_n_future_timesteps=predict_n_future_timesteps\n",
        "            )\n",
        "            if predict_future_param_is_set:\n",
        "                splitted_train, splitted_train_y, splitted_train_target = splitted_train_data\n",
        "                splitted_test, splitted_test_y, splitted_test_target = splitted_test_data\n",
        "            else:\n",
        "                splitted_train, splitted_train_target = splitted_train_data\n",
        "                splitted_test, splitted_test_target = splitted_test_data\n",
        "            if save_result_folder:\n",
        "                if not os.path.exists(save_result_folder):\n",
        "                    os.mkdir(save_result_folder)\n",
        "                additional_folder = f\"{save_result_folder}/w_{windows_size}_o_{overlap}_p_{int(zero_padding)}\"\n",
        "                if not os.path.exists(additional_folder):\n",
        "                    os.mkdir(additional_folder)\n",
        "                dataset_folder = f\"{additional_folder}/{f}\"\n",
        "                if not os.path.exists(dataset_folder):\n",
        "                    os.mkdir(dataset_folder)\n",
        "\n",
        "                # splitted_train = None\n",
        "                pd.DataFrame(splitted_test).to_csv(\n",
        "                    f\"{dataset_folder}/test.csv\", index=False, encoding=\"utf-8\"\n",
        "                )\n",
        "                pd.DataFrame(splitted_test_target).to_csv(\n",
        "                    f\"{dataset_folder}/test_target.csv\", index=False, encoding=\"utf-8\"\n",
        "                )\n",
        "                if len(splitted_train):\n",
        "                    pd.DataFrame(splitted_train).to_csv(\n",
        "                        f\"{dataset_folder}/train.csv\", index=False, encoding=\"utf-8\"\n",
        "                    )\n",
        "                    pd.DataFrame(splitted_train_target).to_csv(\n",
        "                        f\"{dataset_folder}/train_target.csv\",\n",
        "                        index=False,\n",
        "                        encoding=\"utf-8\",\n",
        "                    )\n",
        "                if predict_future_param_is_set:\n",
        "                    pd.DataFrame(splitted_train_y).to_csv(\n",
        "                        f\"{dataset_folder}/train_y.csv\", index=False, encoding=\"utf-8\"\n",
        "                    )\n",
        "                    pd.DataFrame(splitted_test_y).to_csv(\n",
        "                        f\"{dataset_folder}/test_y.csv\",\n",
        "                        index=False,\n",
        "                        encoding=\"utf-8\",\n",
        "                    )\n",
        "            else:\n",
        "                test_target_dict[f] = splitted_test_target\n",
        "                test_data_dict[f] = splitted_test\n",
        "                if len(splitted_train):\n",
        "                    train_target_dict[f] = splitted_train_target\n",
        "                    train_data_dict[f] = splitted_train\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {f}: {e}\")\n",
        "            exceptions[f] = e\n",
        "    return (\n",
        "        train_data_dict,\n",
        "        test_data_dict,\n",
        "        train_target_dict,\n",
        "        test_target_dict,\n",
        "        exceptions,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "pkIzLxDm6XFC",
        "outputId": "bc846c18-1bbe-4caa-effd-7f6453ae468a"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data loading, model training functions\n",
        "\n",
        "#### create_CNN_architecture - creates autoencoder CNN\n",
        "#### load_data - loads data\n",
        "#### train_model - trains model with selected hyperparameters\n",
        "#### load_model - for model retraining\n",
        "#### save_model_data - saves model data \n",
        "#### save_detailed_data - saves info about hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoZwvhEXCgrZ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "from keras import layers, models, callbacks, regularizers, optimizers\n",
        "\n",
        "# from keras.layers import advanced_activations\n",
        "from contextlib import redirect_stdout\n",
        "import absl.logging\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "\n",
        "def create_CNN_architecture(\n",
        "    window_size,\n",
        "    number_of_layers_in_encoder,\n",
        "    encoder_filters,\n",
        "    activation_functions,\n",
        "    kernel_sizes,\n",
        "    batch_normalizations,\n",
        "    max_poolings,\n",
        "    max_pooling_size=2,\n",
        "    allowed_bottleneck_sizes=[16, 24, 32],\n",
        "    **kwargs,\n",
        "):\n",
        "    TIMESTEPS = window_size\n",
        "    num_inputs = 1\n",
        "    input_placeholder = layers.Input(shape=[TIMESTEPS, num_inputs])\n",
        "    encoded = input_placeholder\n",
        "    for i in range(number_of_layers_in_encoder):\n",
        "        encoder_filter = encoder_filters[i]\n",
        "        activation_function = activation_functions[i]\n",
        "        kernel_size = kernel_sizes[i]\n",
        "        batch_normalization = batch_normalizations[i]\n",
        "        max_pooling = max_poolings[i]\n",
        "\n",
        "        encoded = layers.Conv1D(\n",
        "            encoder_filter,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=\"same\",\n",
        "            activation=activation_function,\n",
        "        )(encoded)\n",
        "        if max_pooling:\n",
        "            encoded = layers.MaxPool1D(max_pooling_size)(encoded)\n",
        "        if batch_normalization:\n",
        "            encoded = layers.BatchNormalization()(encoded)\n",
        "    # bottleneck\n",
        "    encoded = layers.Dense(1, activation=\"relu\")(encoded)\n",
        "    encoded = layers.BatchNormalization(name=f\"embedding\")(encoded)\n",
        "    bottleneck_shape = list(encoded.shape)[1]\n",
        "    # print(f'Bottleneck size: {bottleneck_shape}')\n",
        "    if not (bottleneck_shape in allowed_bottleneck_sizes):\n",
        "        raise Exception(f\"Wrong bottleneck shape: {bottleneck_shape}\")\n",
        "\n",
        "    decoded = encoded\n",
        "\n",
        "    for i in reversed(range(number_of_layers_in_encoder)):\n",
        "        encoder_filter = encoder_filters[i]\n",
        "        activation_function = activation_functions[i]\n",
        "        kernel_size = kernel_sizes[i]\n",
        "        batch_normalization = batch_normalizations[i]\n",
        "        decoded = layers.Conv1DTranspose(\n",
        "            encoder_filter,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=\"same\",\n",
        "            activation=activation_function,\n",
        "        )(decoded)\n",
        "        if batch_normalization:\n",
        "            decoded = layers.BatchNormalization()(decoded)\n",
        "        if max_pooling:\n",
        "            decoded = layers.UpSampling1D(max_pooling_size)(decoded)\n",
        "\n",
        "    decoded = layers.Conv1DTranspose(\n",
        "        filters=1, kernel_size=kernel_size, padding=\"same\"\n",
        "    )(decoded)\n",
        "\n",
        "    autoencoder = models.Model(inputs=input_placeholder, outputs=decoded)\n",
        "    return autoencoder, bottleneck_shape\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_data(main_data_folder, exclude_dataset_for_testing, predict_n_future_steps=False):\n",
        "    data_folders = os.listdir(main_data_folder)\n",
        "    train_data_df = pd.DataFrame()\n",
        "    test_data_df = pd.DataFrame()\n",
        "    exceptions = {}\n",
        "    train_length = 0\n",
        "    test_length = 0\n",
        "    print(f\"Total datasets {len(data_folders)}\")\n",
        "    for f in data_folders:\n",
        "        try:\n",
        "            test_df = pd.read_csv(f\"{main_data_folder}/{f}/test.csv\")\n",
        "            \n",
        "            if f == exclude_dataset_for_testing:\n",
        "                continue\n",
        "            else:\n",
        "                test_length += len(test_df)\n",
        "                train_df = pd.read_csv(f\"{main_data_folder}/{f}/train.csv\")\n",
        "            train_length += len(train_df)\n",
        "            train_data_df = pd.concat(\n",
        "                [train_data_df, train_df], ignore_index=True\n",
        "            )  # train_data_df.append(train_df, ignore_index=True)\n",
        "            test_data_df = pd.concat(\n",
        "                [test_data_df, test_df], ignore_index=True\n",
        "            )  # test_data_df.append(test_df, ignore_index=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            exceptions[f] = e\n",
        "    assert train_length == len(\n",
        "        train_data_df\n",
        "    ), \"Not all training data was appended to final training set\"\n",
        "    assert test_length == len(\n",
        "        test_data_df\n",
        "    ), \"Not all testing data was appended to final testing set\"\n",
        "    if not predict_n_future_steps:\n",
        "        return train_data_df, test_data_df, exceptions\n",
        "    else:\n",
        "        train_y_df = pd.read_csv(f\"{main_data_folder}/{f}/train_y.csv\")\n",
        "        test_y_df = pd.read_csv(f\"{main_data_folder}/{f}/train_y.csv\")\n",
        "        return train_data_df, test_data_df, train_y_df, test_y_df, exceptions\n",
        "        \n",
        "\n",
        "def compile_model(model, optimizer, loss=\"mse\"):\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    model_name,\n",
        "    train_data,\n",
        "    test_data,\n",
        "    main_model_folder,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    use_early_stopping=True,\n",
        "    train_data_shifted=None,\n",
        "    test_data_shifted=None\n",
        "):\n",
        "    train_data_y = train_data_shifted if train_data_shifted is not None else train_data\n",
        "    test_data_y = test_data_shifted if test_data_shifted is not None else test_data\n",
        "    if use_early_stopping:\n",
        "        history = model.fit(\n",
        "            train_data,\n",
        "            train_data_y,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(test_data, test_data_y),\n",
        "            callbacks=[\n",
        "                callbacks.ModelCheckpoint(\n",
        "                    f\"{main_model_folder}/\"\n",
        "                    + model_name\n",
        "                    + f\"/callbacks\"\n",
        "                    + \"/epoch{epoch:02d}-loss{val_loss:.3f}.tf\"\n",
        "                ),\n",
        "                callbacks.ModelCheckpoint(\n",
        "                    f\"{main_model_folder}/\" + model_name + f\"/callbacks\" + \"/best.tf\",\n",
        "                    save_best_only=True,\n",
        "                ),\n",
        "                callbacks.EarlyStopping(monitor='val_loss',\n",
        "                              min_delta=0,\n",
        "                              patience=10,\n",
        "                              verbose=1, mode='auto')\n",
        "            ],\n",
        "            verbose=1,\n",
        "        )\n",
        "    else:\n",
        "        history = model.fit(\n",
        "            train_data,\n",
        "            train_data,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(test_data, test_data),\n",
        "            callbacks=[\n",
        "                callbacks.ModelCheckpoint(\n",
        "                    f\"{main_model_folder}/\"\n",
        "                    + model_name\n",
        "                    + f\"/callbacks\"\n",
        "                    + \"/epoch{epoch:02d}-loss{val_loss:.3f}.tf\"\n",
        "                ),\n",
        "                callbacks.ModelCheckpoint(\n",
        "                    f\"{main_model_folder}/\" + model_name + f\"/callbacks\" + \"/best.tf\",\n",
        "                    save_best_only=True,\n",
        "                )\n",
        "\n",
        "            ],\n",
        "            verbose=1,\n",
        "        )\n",
        "\n",
        "    return history\n",
        "\n",
        "def load_model(model_path):\n",
        "    K.set_learning_phase(0)\n",
        "    model_path = f'{model_path}/best.tf'\n",
        "    new_model = tf.keras.models.load_model(model_path)\n",
        "    return new_model\n",
        "    \n",
        "\n",
        "def save_model_data(model, history, main_model_folder):\n",
        "    def save_model_summary(model, path_to_save):\n",
        "        with open(f\"{path_to_save}/model_summary.txt\", \"w\") as f:\n",
        "            with redirect_stdout(f):\n",
        "                model.summary()\n",
        "        pd.DataFrame.from_dict(history.history).to_csv(f\"{path_to_save}/history.csv\")\n",
        "\n",
        "    if not os.path.exists(main_model_folder):\n",
        "        os.mkdir(main_model_folder)\n",
        "    with open(f'{main_model_folder}' + '/model_structure.json', mode='w') as ofile:\n",
        "        ofile.write(model.to_json())\n",
        "    save_model_summary(model, main_model_folder)\n",
        "    \n",
        "\n",
        "def save_detailed_data(main_model_folder, **kwargs):\n",
        "    df = pd.DataFrame()\n",
        "    for k, v in kwargs.items():\n",
        "        df[str(k)] = [str(v)]\n",
        "    df= df.T\n",
        "    df.to_csv(f'{main_model_folder}/details.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing data (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exclude_dataset_for_testing = \"InsectWingbeat\"\n",
        "windows_size = 128\n",
        "overlap = 64\n",
        "zero_padding = False\n",
        "predict_n_future_timesteps = -1\n",
        "\n",
        "(\n",
        "    train_data_dict,\n",
        "    test_data_dict,\n",
        "    train_target_dict,\n",
        "    test_target_dict,\n",
        "    exceptions,\n",
        ") = load_preprocessed_datasets_and_processe(\n",
        "    \"processed_datasets\", exclude_dataset_for_testing,\n",
        "    save_result_folder='fully_processed_data', windows_size=windows_size, overlap=overlap, zero_padding=zero_padding,\n",
        "    predict_n_future_timesteps=predict_n_future_timesteps\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining model architecture, loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_nJZzdVRfPd"
      },
      "outputs": [],
      "source": [
        "unique_model_name = 'model_x_3'\n",
        "\n",
        "model_arch = {\n",
        "    \"name\": unique_model_name,\n",
        "    \"window_size\": 128,\n",
        "    \"number_of_layers_in_encoder\": 2,\n",
        "    \"input\": 128,\n",
        "    \"encoder_filters\": [128, 64],\n",
        "    \"kernel_sizes\": [13, 7],\n",
        "    \"activation_functions\": [\"relu\"] * 2,\n",
        "    \"batch_normalizations\": [False] * 2,\n",
        "    \"max_poolings\": [True] * 2,\n",
        "}\n",
        "model, emb = create_CNN_architecture(**model_arch)\n",
        "#save_detailed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evmGs4-QCpyu",
        "outputId": "48c724a2-4cf2-4516-ed08-a19b4b1662c8"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiTmIJ5BCx-L"
      },
      "outputs": [],
      "source": [
        "info = {}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALOLw1AvDE4Z",
        "outputId": "950db2cb-c769-47f5-f2ee-c9284256ce3d"
      },
      "outputs": [],
      "source": [
        "# Set parameters if they are not already set\n",
        "try:\n",
        "    info['windows_size'] = windows_size\n",
        "except:\n",
        "    windows_size = 128\n",
        "    info['windows_size'] = windows_size\n",
        "    \n",
        "try:\n",
        "    info['overlap'] = overlap\n",
        "except:\n",
        "    overlap = 64\n",
        "    info['overlap'] = overlap\n",
        "    \n",
        "try:\n",
        "    info['zero_padding'] = zero_padding\n",
        "except:\n",
        "    zero_padding = False\n",
        "    info['zero_padding'] = zero_padding\n",
        "    \n",
        "try:\n",
        "    info['predict_n_future_timesteps'] = predict_n_future_timesteps\n",
        "except:\n",
        "    predict_n_future_timesteps = -1\n",
        "    info['predict_n_future_timesteps'] = predict_n_future_timesteps\n",
        "\n",
        "exclude_dataset_for_testing = \"InsectSound\"\n",
        "folder_name = f\"fully_processed_data/w_{windows_size}_o_{overlap}_p_{int(zero_padding)}\"\n",
        "train_data_df, test_data_df, exceptions = load_data(\n",
        "    folder_name, exclude_dataset_for_testing\n",
        ")\n",
        "train_data = train_data_df.values\n",
        "train_data = train_data.reshape(train_data.shape[0], train_data.shape[1], 1)\n",
        "test_data = test_data_df.values\n",
        "test_data = test_data.reshape(test_data.shape[0], test_data.shape[1], 1)\n",
        "main_model_folder = 'trained_models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main_model_folder = 'trained_models'\n",
        "BATCH_SIZE = 32\n",
        "learning_rate = .00001\n",
        "opt = optimizers.Adam(learning_rate=learning_rate)\n",
        "epochs = 100\n",
        "loss = 'mse'\n",
        "\n",
        "info['batch_size'] = BATCH_SIZE\n",
        "info['learning_rate']  = learning_rate\n",
        "info['opt'] =  opt\n",
        "info['epochs'] = epochs\n",
        "\n",
        "info.updated(model_arch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SvT2sZ-CqI2",
        "outputId": "17e0ef32-bdf6-4569-eae5-118cb7c962f1"
      },
      "outputs": [],
      "source": [
        "k = model_arch['name']\n",
        "#model\n",
        "# showing model info\n",
        "print(f'INFO:')\n",
        "for k, v in model_arch.items():\n",
        "    print(f'{k}: {v}')\n",
        "\n",
        "model.compile(optimizer=opt, loss=loss)\n",
        "folder_name = f'{main_model_folder}/{k}'\n",
        "history = train_model(model, k, train_data, test_data, main_model_folder, epochs=epochs)\n",
        "save_model_data(model, history, folder_name)\n",
        "save_detailed_data(folder_name, **info)\n",
        "re = model.evaluate(test_data)\n",
        "hist_df = pd.DataFrame.from_dict(history.history)\n",
        "lowest_test_val_loss = hist_df.iloc[hist_df['val_loss'].argmin()]\n",
        "print(f'Model {k} results {re}')\n",
        "vals = lowest_test_val_loss.values\n",
        "print(f'Val loss: train {vals[0]} test: {vals[1]}')\n",
        "print(\"========================= Finished training model =========================\")\n",
        "print('\\n')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retraining loaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = \"\"\n",
        "model = load_model(model_path)\n",
        "k = 'model_name'\n",
        "\n",
        "folder_name = f'{main_model_folder}/{k}_retrained'\n",
        "history = train_model(model, k, train_data, test_data, main_model_folder, epochs=epochs)\n",
        "save_model_data(model, history, folder_name)\n",
        "re = model.evaluate(test_data)\n",
        "hist_df = pd.DataFrame.from_dict(history.history)\n",
        "lowest_test_val_loss = hist_df.iloc[hist_df['val_loss'].argmin()]\n",
        "print(f'Model {k} results {re}')\n",
        "vals = lowest_test_val_loss.values\n",
        "print(f'Val loss: train {vals[0]} test: {vals[1]}')\n",
        "print(\"========================= Finished training model =========================\")\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e0bb84656e3b8df8dc5880187da3fc23c20d4facc0eb3f3536420f5d579c6b20"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
