{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "\n",
        "os.getcwd()\n",
        "os.chdir('gdrive/MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TXvI3voCYo0",
        "outputId": "ed413919-acb6-465b-9761-18acbbda0de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "from keras import layers, models, callbacks, regularizers, optimizers\n",
        "\n",
        "# from keras.layers import advanced_activations\n",
        "from contextlib import redirect_stdout\n",
        "import absl.logging\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "\n",
        "def create_CNN_architecture(\n",
        "    window_size,\n",
        "    number_of_layers_in_encoder,\n",
        "    encoder_filters,\n",
        "    activation_functions,\n",
        "    kernel_sizes,\n",
        "    batch_normalizations,\n",
        "    max_poolings,\n",
        "    max_pooling_size=2,\n",
        "    allowed_bottleneck_sizes=[16, 24, 32],\n",
        "    **kwargs,\n",
        "):\n",
        "    TIMESTEPS = window_size\n",
        "    num_inputs = 1\n",
        "    input_placeholder = layers.Input(shape=[TIMESTEPS, num_inputs])\n",
        "    encoded = input_placeholder\n",
        "    for i in range(number_of_layers_in_encoder):\n",
        "        encoder_filter = encoder_filters[i]\n",
        "        activation_function = activation_functions[i]\n",
        "        kernel_size = kernel_sizes[i]\n",
        "        batch_normalization = batch_normalizations[i]\n",
        "        max_pooling = max_poolings[i]\n",
        "\n",
        "        encoded = layers.Conv1D(\n",
        "            encoder_filter,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=\"same\",\n",
        "            activation=activation_function,\n",
        "        )(encoded)\n",
        "        if max_pooling:\n",
        "            encoded = layers.MaxPool1D(max_pooling_size)(encoded)\n",
        "        if batch_normalization:\n",
        "            encoded = layers.BatchNormalization()(encoded)\n",
        "    # bottleneck\n",
        "    encoded = layers.Dense(1, activation=\"relu\")(encoded)\n",
        "    encoded = layers.BatchNormalization(name=f\"embedding\")(encoded)\n",
        "    bottleneck_shape = list(encoded.shape)[1]\n",
        "    # print(f'Bottleneck size: {bottleneck_shape}')\n",
        "    if not (bottleneck_shape in allowed_bottleneck_sizes):\n",
        "        raise Exception(f\"Wrong bottleneck shape: {bottleneck_shape}\")\n",
        "\n",
        "    decoded = encoded\n",
        "\n",
        "    for i in reversed(range(number_of_layers_in_encoder)):\n",
        "        encoder_filter = encoder_filters[i]\n",
        "        activation_function = activation_functions[i]\n",
        "        kernel_size = kernel_sizes[i]\n",
        "        batch_normalization = batch_normalizations[i]\n",
        "        decoded = layers.Conv1DTranspose(\n",
        "            encoder_filter,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=\"same\",\n",
        "            activation=activation_function,\n",
        "        )(decoded)\n",
        "        if max_pooling:\n",
        "            decoded = layers.UpSampling1D(max_pooling_size)(decoded)\n",
        "        if batch_normalization:\n",
        "            decoded = layers.BatchNormalization()(decoded)\n",
        "\n",
        "    decoded = layers.Conv1DTranspose(\n",
        "        filters=1, kernel_size=kernel_size, padding=\"same\"\n",
        "    )(decoded)\n",
        "\n",
        "    autoencoder = models.Model(inputs=input_placeholder, outputs=decoded)\n",
        "    return autoencoder, bottleneck_shape\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_data(main_data_folder, exclude_dataset_for_testing):\n",
        "    data_folders = os.listdir(main_data_folder)\n",
        "    train_data_df = pd.DataFrame()\n",
        "    test_data_df = pd.DataFrame()\n",
        "    exceptions = {}\n",
        "    train_length = 0\n",
        "    test_length = 0\n",
        "    print(f\"Total datasets {len(data_folders)}\")\n",
        "    for f in data_folders:\n",
        "        try:\n",
        "            test_df = pd.read_csv(f\"{main_data_folder}/{f}/test.csv\")\n",
        "            \n",
        "            if f == exclude_dataset_for_testing:\n",
        "                continue\n",
        "            else:\n",
        "                test_length += len(test_df)\n",
        "                train_df = pd.read_csv(f\"{main_data_folder}/{f}/train.csv\")\n",
        "            train_length += len(train_df)\n",
        "            train_data_df = pd.concat(\n",
        "                [train_data_df, train_df], ignore_index=True\n",
        "            )  # train_data_df.append(train_df, ignore_index=True)\n",
        "            test_data_df = pd.concat(\n",
        "                [test_data_df, test_df], ignore_index=True\n",
        "            )  # test_data_df.append(test_df, ignore_index=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            exceptions[f] = e\n",
        "    assert train_length == len(\n",
        "        train_data_df\n",
        "    ), \"Not all training data was appended to final training set\"\n",
        "    assert test_length == len(\n",
        "        test_data_df\n",
        "    ), \"Not all testing data was appended to final testing set\"\n",
        "    return train_data_df, test_data_df, exceptions\n",
        "\n",
        "def compile_model(model, optimizer, loss=\"mse\"):\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    model_name,\n",
        "    train_data,\n",
        "    test_data,\n",
        "    main_model_folder,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "):\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(test_data, test_data),\n",
        "        callbacks=[\n",
        "            callbacks.ModelCheckpoint(\n",
        "                f\"{main_model_folder}/\"\n",
        "                + model_name\n",
        "                + f\"/callbacks\"\n",
        "                + \"/epoch{epoch:02d}-loss{val_loss:.3f}.tf\"\n",
        "            ),\n",
        "            callbacks.ModelCheckpoint(\n",
        "                f\"{main_model_folder}/\" + model_name + f\"/callbacks\" + \"/best.tf\",\n",
        "                save_best_only=True,\n",
        "            ),\n",
        "            callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=5, verbose=1)\n",
        "        ],\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def save_model_data(model, history, main_model_folder):\n",
        "    def save_model_summary(model, path_to_save):\n",
        "        with open(f\"{path_to_save}/model_summary.txt\", \"w\") as f:\n",
        "            with redirect_stdout(f):\n",
        "                model.summary()\n",
        "        pd.DataFrame.from_dict(history.history).to_csv(f\"{path_to_save}/history.csv\")\n",
        "\n",
        "    if not os.path.exists(main_model_folder):\n",
        "        os.mkdir(main_model_folder)\n",
        "    with open(f'{main_model_folder}' + '/model_structure.json', mode='w') as ofile:\n",
        "        ofile.write(model.to_json())\n",
        "    save_model_summary(model, main_model_folder)\n"
      ],
      "metadata": {
        "id": "PoZwvhEXCgrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "yBvCm5adCCHb",
        "outputId": "359aeacd-8a2f-47d2-9fc9-533aa9a74237"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_7 = {\\n    \"name\": \"model_8\",\\n    \"window_size\": 128,\\n    \"number_of_layers_in_encoder\": 4,\\n    \"input\": 128,\\n    \"encoder_filters\": [512, 256, 128],\\n    \"kernel_sizes\": [23, 13, 3],\\n    \"activation_functions\": [\"relu\"] * 3,\\n    \"batch_normalizations\": [False] * 3,\\n    \"max_poolings\": [True] * 3,\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "from keras import layers, models, callbacks, regularizers, optimizers\n",
        "#from keras.layers import advanced_activations\n",
        "\n",
        "\n",
        "# Create graph structure.\n",
        "TIMESTEPS = 128\n",
        "num_inputs = 1\n",
        "input_placeholder = layers.Input(shape=[TIMESTEPS, num_inputs])\n",
        "# Encoder.\n",
        "encoded = layers.Conv1D(512, kernel_size=23, padding=\"same\", activation='relu')(input_placeholder)\n",
        "encoded = layers.MaxPool1D(2)(encoded)\n",
        "encoded = layers.BatchNormalization()(encoded)\n",
        "encoded = layers.Conv1D(\n",
        "            filters=256, kernel_size=13, padding=\"same\", activation='relu')(encoded)\n",
        "#encoded = layers.MaxPool1D(2)(encoded)\n",
        "encoded = layers.MaxPool1D(2)(encoded)\n",
        "encoded = layers.BatchNormalization()(encoded)\n",
        "encoded = layers.Conv1D(\n",
        "            filters=256, kernel_size=7, padding=\"same\", activation='relu')(encoded)\n",
        "encoded = layers.MaxPool1D(2)(encoded)\n",
        "encoded = layers.BatchNormalization()(encoded)\n",
        "encoded = layers.Conv1D(\n",
        "            filters=128, kernel_size=3, padding=\"same\", activation='relu')(encoded)\n",
        "encoded = layers.Dense(1, activation='relu')(encoded)\n",
        "encoded = layers.BatchNormalization(name='embedding')(encoded)\n",
        "# Decoder.\n",
        "#decoded = layers.UpSampling1D(2)(encoded)\n",
        "decoded = layers.Conv1DTranspose(128, kernel_size=3, padding=\"same\", activation='relu')(encoded)\n",
        "decoded = layers.Conv1DTranspose(256, kernel_size=7, padding=\"same\", activation=\"relu\")(decoded)\n",
        "decoded = layers.BatchNormalization()(decoded)\n",
        "#decoded = layers.Conv1DTranspose(256, kernel_size=7, padding=\"same\", activation=\"relu\")(decoded)\n",
        "decoded = layers.UpSampling1D(2)(decoded)\n",
        "decoded = layers.Conv1DTranspose(256, kernel_size=7, padding=\"same\", activation=\"relu\")(decoded)\n",
        "decoded = layers.BatchNormalization()(decoded)\n",
        "#decoded = layers.UpSampling1D(2)(decoder)\n",
        "#decoded = layers.BatchNormalization()(decoded)\n",
        "decoded = layers.UpSampling1D(2)(decoded)\n",
        "decoded = layers.Conv1DTranspose(512, kernel_size=13, padding=\"same\", activation=\"relu\")(decoded)\n",
        "#decoded = layers.UpSampling1D(2)(decoder)\n",
        "decoded = layers.BatchNormalization()(decoded)\n",
        "#decoded = layers.Conv1DTranspose(512, kernel_size=23, padding=\"same\", activation=\"relu\")(decoded)\n",
        "decoded = layers.UpSampling1D(2)(decoded)\n",
        "#decoded = layers.UpSampling1D(2)(decoded)\n",
        "#decoded = layers.BatchNormalization()(decoded)\n",
        "#decoded = layers.Conv1DTranspose(2, kernel_size=5, padding=\"same\", strides=2, activation=\"relu\")(decoded)\n",
        "decoded = layers.Conv1DTranspose(filters=1, kernel_size=13, padding=\"same\")(decoded)\n",
        "\n",
        "encoder = models.Model(inputs=input_placeholder, outputs=encoded)\n",
        "model = models.Model(inputs=input_placeholder, outputs=decoded)\n",
        "\n",
        "\"\"\"model_7 = {\n",
        "    \"name\": \"model_8\",\n",
        "    \"window_size\": 128,\n",
        "    \"number_of_layers_in_encoder\": 4,\n",
        "    \"input\": 128,\n",
        "    \"encoder_filters\": [512, 256, 128],\n",
        "    \"kernel_sizes\": [23, 13, 3],\n",
        "    \"activation_functions\": [\"relu\"] * 3,\n",
        "    \"batch_normalizations\": [False] * 3,\n",
        "    \"max_poolings\": [True] * 3,\n",
        "}\"\"\"\n",
        "\n",
        "#model, emb = create_CNN_architecture(**model_7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evmGs4-QCpyu",
        "outputId": "29a8410a-a1f0-434c-870b-12162c044727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_43\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_24 (InputLayer)       [(None, 128, 1)]          0         \n",
            "                                                                 \n",
            " conv1d_88 (Conv1D)          (None, 128, 512)          12288     \n",
            "                                                                 \n",
            " max_pooling1d_68 (MaxPoolin  (None, 64, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_130 (Ba  (None, 64, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv1d_89 (Conv1D)          (None, 64, 256)           1704192   \n",
            "                                                                 \n",
            " max_pooling1d_69 (MaxPoolin  (None, 32, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_131 (Ba  (None, 32, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv1d_90 (Conv1D)          (None, 32, 256)           459008    \n",
            "                                                                 \n",
            " max_pooling1d_70 (MaxPoolin  (None, 16, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_132 (Ba  (None, 16, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv1d_91 (Conv1D)          (None, 16, 128)           98432     \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 16, 1)             129       \n",
            "                                                                 \n",
            " embedding (BatchNormalizati  (None, 16, 1)            4         \n",
            " on)                                                             \n",
            "                                                                 \n",
            " conv1d_transpose_119 (Conv1  (None, 16, 128)          512       \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " conv1d_transpose_120 (Conv1  (None, 16, 256)          229632    \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " batch_normalization_133 (Ba  (None, 16, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " up_sampling1d_41 (UpSamplin  (None, 32, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_transpose_121 (Conv1  (None, 32, 256)          459008    \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " batch_normalization_134 (Ba  (None, 32, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " up_sampling1d_42 (UpSamplin  (None, 64, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_transpose_122 (Conv1  (None, 64, 512)          1704448   \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " batch_normalization_135 (Ba  (None, 64, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " up_sampling1d_43 (UpSamplin  (None, 128, 512)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_transpose_123 (Conv1  (None, 128, 1)           6657      \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,682,502\n",
            "Trainable params: 4,678,404\n",
            "Non-trainable params: 4,098\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_model_folder = 'trained_models'\n",
        "#os.mkdir(f'{main_model_folder}/')\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "PiTmIJ5BCx-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exclude_dataset_for_testing = \"InsectSound\"\n",
        "folder_name = \"fully_processed_data/w_128_o_64_p_0\"\n",
        "train_data_df, test_data_df, exceptions = load_data(\n",
        "    folder_name, exclude_dataset_for_testing\n",
        ")\n",
        "train_data = train_data_df.values\n",
        "train_data = train_data.reshape(train_data.shape[0], train_data.shape[1], 1)\n",
        "test_data = test_data_df.values\n",
        "test_data = test_data.reshape(test_data.shape[0], test_data.shape[1], 1)\n",
        "main_model_folder = 'trained_models'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALOLw1AvDE4Z",
        "outputId": "f2c4d92a-fcbc-4bc5-c63c-355e622f6ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total datasets 97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_arch = 'model_11'\n",
        "model, embed = model, 32#create_CNN_architecture(**model_arch)\n",
        "opt = optimizers.Adam(learning_rate=.0001)\n",
        "epochs = 100\n",
        "k = 'model_11'\n",
        "embedding_size = embed\n",
        "model_arch = None\n",
        "print(f'Model iteration: 0 name: {k}')\n",
        "if model_arch is not None:\n",
        "  number_of_layers_in_encoder = model_arch['number_of_layers_in_encoder']\n",
        "  encoder_filters = model_arch['encoder_filters']\n",
        "  kernel_sizes = model_arch['kernel_sizes']\n",
        "  print(f'INFO: Layers: {number_of_layers_in_encoder} | embedding size {embedding_size} | Kernel filters {encoder_filters} | Kernel sizes {kernel_sizes}')\n",
        "model.compile(optimizer=opt, loss='mse')\n",
        "folder_name = f'{main_model_folder}/{k}'\n",
        "history = train_model(model, k, train_data, test_data, main_model_folder, epochs=epochs)\n",
        "save_model_data(model, history, folder_name)\n",
        "re = model.evaluate(test_data)\n",
        "hist_df = pd.DataFrame.from_dict(history.history)\n",
        "lowest_test_val_loss = hist_df.iloc[hist_df['val_loss'].argmin()]\n",
        "print(f'Model {k} results {re}')\n",
        "vals = lowest_test_val_loss.values\n",
        "print(f'Val loss: train {vals[0]} test: {vals[1]}')\n",
        "print(\"========================= Finished training model =========================\")\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SvT2sZ-CqI2",
        "outputId": "4efae5cb-8eb4-46b7-dc4f-20c524ce6e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model iteration: 0 name: model_11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHbcSyqtHZ74"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}