{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "\n",
        "os.getcwd()\n",
        "os.chdir('gdrive/MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TXvI3voCYo0",
        "outputId": "73fbe942-aa2a-4e5b-bba5-8ddbc7aa14f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "from itertools import chain\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "from scipy.io import arff\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def splitting_data(\n",
        "    df, df_target=None, take_time_stamps=124, overlap=62, zero_padding=True\n",
        "):\n",
        "    def windowed_view_adj(\n",
        "        arr, window=take_time_stamps, overlap=overlap, zero_padding=zero_padding\n",
        "    ):\n",
        "        windows = windowed_view(arr, window, overlap)\n",
        "        if zero_padding:\n",
        "            re = add_zero_padding(arr, window, overlap)\n",
        "            return np.append(windows, re, axis=0)\n",
        "        return windows\n",
        "\n",
        "    def calculate_number_of_created_samples(\n",
        "        arr, window=take_time_stamps, overlap=overlap, zero_padding=zero_padding\n",
        "    ):\n",
        "        window_step = window - overlap\n",
        "        new_shape = ((arr.shape[-1] - overlap) // window_step, window)\n",
        "        return new_shape[0] + 1 if zero_padding else new_shape[0]\n",
        "\n",
        "    vals = df.values\n",
        "    vals_shape = vals.shape\n",
        "    if vals_shape[1] >= take_time_stamps:\n",
        "        if df_target is None:\n",
        "            data = list(map(windowed_view_adj, vals))\n",
        "            return data, None\n",
        "        else:\n",
        "            targ_data = df_target.values\n",
        "            temp_re = [\n",
        "                (\n",
        "                    [\n",
        "                        windowed_view_adj(l),\n",
        "                        np.array(list(d) * calculate_number_of_created_samples(l)),\n",
        "                    ]\n",
        "                )\n",
        "                for l, d in zip(vals, targ_data)\n",
        "            ]\n",
        "            data, data_target = zip(*temp_re)\n",
        "            data = np.array(data)\n",
        "            dat_shape = data.shape\n",
        "            data = data.reshape(dat_shape[0] * dat_shape[1], dat_shape[-1])\n",
        "            data_target = list(chain(*data_target))\n",
        "            assert data.shape[0] == len(\n",
        "                data_target\n",
        "            ), \"Target and data rows are different size!\"\n",
        "            return data, data_target\n",
        "\n",
        "    else:\n",
        "        print(\"Not enough samples\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def windowed_view(arr, window, overlap):\n",
        "    arr = np.asarray(arr)\n",
        "    window_step = window - overlap\n",
        "    new_shape = arr.shape[:-1] + ((arr.shape[-1] - overlap) // window_step, window)\n",
        "    new_strides = arr.strides[:-1] + (window_step * arr.strides[-1],) + arr.strides[-1:]\n",
        "    return as_strided(arr, shape=new_shape, strides=new_strides)\n",
        "\n",
        "\n",
        "def add_zero_padding(arr, window, overlap):\n",
        "    # need_zeros = len(arr)\n",
        "    array_len = len(arr)\n",
        "    window_step = window - overlap\n",
        "    number_of_els = (arr.shape[-1] - overlap) // window_step\n",
        "    take_ind = number_of_els * window_step\n",
        "    number_of_left_elements = array_len - take_ind\n",
        "    padded_arr = np.array(\n",
        "        list(arr[take_ind:]) + (window - number_of_left_elements) * [0]\n",
        "    ).reshape(1, window)\n",
        "    assert padded_arr.shape == (\n",
        "        1,\n",
        "        window,\n",
        "    ), f\"Wrong dimensions after zero padding, expected (1, {window}), got {padded_arr.shape}\"\n",
        "    return padded_arr\n",
        "\n",
        "\n",
        "def load_preprocessed_datasets_and_processe(\n",
        "    main_data_folder,\n",
        "    exclude_dataset_for_testing,\n",
        "    save_result_folder=None,\n",
        "    windows_size=128,\n",
        "    overlap=64,\n",
        "    zero_padding=False,\n",
        "):\n",
        "    train_data_dict = {}\n",
        "    test_data_dict = {}\n",
        "    train_target_dict = {}\n",
        "    test_target_dict = {}\n",
        "    data_folders = os.listdir(main_data_folder)\n",
        "    exceptions = {}\n",
        "    print(f\"Total datasets {len(data_folders)}\")\n",
        "    for f in data_folders:\n",
        "        try:\n",
        "            test_df = pd.read_csv(f\"{main_data_folder}/{f}/test.csv\")\n",
        "            target_test_df = pd.read_csv(f\"{main_data_folder}/{f}/test_target.csv\")\n",
        "            test_shape = test_df.shape\n",
        "            if test_shape[1] < windows_size:\n",
        "                exceptions[\n",
        "                    f\n",
        "                ] = f\"Not enough samples in row, found {test_shape[1]}, expected (window size) {windows_size}\"\n",
        "                continue\n",
        "            if f == exclude_dataset_for_testing:\n",
        "                splitted_train, splitted_train_target = pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "            else:\n",
        "                train_df = pd.read_csv(f\"{main_data_folder}/{f}/train.csv\")\n",
        "                test_df = pd.read_csv(f\"{main_data_folder}/{f}/test.csv\")\n",
        "                target_train_df = pd.read_csv(\n",
        "                    f\"{main_data_folder}/{f}/train_target.csv\"\n",
        "                )\n",
        "\n",
        "                splitted_train, splitted_train_target = splitting_data(\n",
        "                    train_df,\n",
        "                    target_train_df,\n",
        "                    take_time_stamps=windows_size,\n",
        "                    overlap=overlap,\n",
        "                    zero_padding=zero_padding,\n",
        "                )\n",
        "            splitted_test, splitted_test_target = splitting_data(\n",
        "                test_df,\n",
        "                target_test_df,\n",
        "                take_time_stamps=windows_size,\n",
        "                overlap=overlap,\n",
        "                zero_padding=zero_padding,\n",
        "            )\n",
        "            if save_result_folder:\n",
        "                if not os.path.exists(save_result_folder):\n",
        "                    os.mkdir(save_result_folder)\n",
        "                additional_folder = f\"{save_result_folder}/w_{windows_size}_o_{overlap}_p_{int(zero_padding)}\"\n",
        "                if not os.path.exists(additional_folder):\n",
        "                    os.mkdir(additional_folder)\n",
        "                dataset_folder = f\"{additional_folder}/{f}\"\n",
        "                if not os.path.exists(dataset_folder):\n",
        "                    os.mkdir(dataset_folder)\n",
        "\n",
        "                # splitted_train = None\n",
        "                pd.DataFrame(splitted_test).to_csv(\n",
        "                    f\"{dataset_folder}/test.csv\", index=False, encoding=\"utf-8\"\n",
        "                )\n",
        "                pd.DataFrame(splitted_test_target).to_csv(\n",
        "                    f\"{dataset_folder}/test_target.csv\", index=False, encoding=\"utf-8\"\n",
        "                )\n",
        "                if len(splitted_train):\n",
        "                    pd.DataFrame(splitted_train).to_csv(\n",
        "                        f\"{dataset_folder}/train.csv\", index=False, encoding=\"utf-8\"\n",
        "                    )\n",
        "                    pd.DataFrame(splitted_train_target).to_csv(\n",
        "                        f\"{dataset_folder}/train_target.csv\",\n",
        "                        index=False,\n",
        "                        encoding=\"utf-8\",\n",
        "                    )\n",
        "            else:\n",
        "                test_target_dict[f] = splitted_test_target\n",
        "                test_data_dict[f] = splitted_test\n",
        "                if len(splitted_train):\n",
        "                    train_target_dict[f] = splitted_train_target\n",
        "                    train_data_dict[f] = splitted_train\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {f}: {e}\")\n",
        "            exceptions[f] = e\n",
        "    return (\n",
        "        train_data_dict,\n",
        "        test_data_dict,\n",
        "        train_target_dict,\n",
        "        test_target_dict,\n",
        "        exceptions,\n",
        "    )"
      ],
      "metadata": {
        "id": "oAob49FR52EG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exclude_dataset_for_testing = \"InsectSound\"\n",
        "(\n",
        "    train_data_dict,\n",
        "    test_data_dict,\n",
        "    train_target_dict,\n",
        "    test_target_dict,\n",
        "    exceptions,\n",
        ") = load_preprocessed_datasets_and_processe(\n",
        "    \"processed_datasets\", exclude_dataset_for_testing,\n",
        "    save_result_folder='fully_processed_data', windows_size=128, overlap=64\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkIzLxDm6XFC",
        "outputId": "0f7cfbbf-cb6b-44cf-e98b-d1fd3bc31a94"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total datasets 120\n",
            "Error with ItalyPowerDemand: [Errno 2] No such file or directory: 'processed_datasets/ItalyPowerDemand/test.csv'\n",
            "Error with InsectWingbeatSound: [Errno 2] No such file or directory: 'processed_datasets/InsectWingbeatSound/test.csv'\n",
            "Error with InsectEPGSmallTrain: [Errno 2] No such file or directory: 'processed_datasets/InsectEPGSmallTrain/test.csv'\n",
            "Error with InsectEPGRegularTrain: [Errno 2] No such file or directory: 'processed_datasets/InsectEPGRegularTrain/test.csv'\n",
            "Error with InlineSkate: [Errno 2] No such file or directory: 'processed_datasets/InlineSkate/test.csv'\n",
            "Error with HouseTwenty: [Errno 2] No such file or directory: 'processed_datasets/HouseTwenty/test.csv'\n",
            "Error with Herring: [Errno 2] No such file or directory: 'processed_datasets/Herring/test.csv'\n",
            "Error with Haptics: [Errno 2] No such file or directory: 'processed_datasets/Haptics/test.csv'\n",
            "Error with HandOutlines: [Errno 2] No such file or directory: 'processed_datasets/HandOutlines/test.csv'\n",
            "Error with Ham: [Errno 2] No such file or directory: 'processed_datasets/Ham/test.csv'\n",
            "Error with GunPointOldVersusYoung: [Errno 2] No such file or directory: 'processed_datasets/GunPointOldVersusYoung/test.csv'\n",
            "Error with GunPointMaleVersusFemale: [Errno 2] No such file or directory: 'processed_datasets/GunPointMaleVersusFemale/test.csv'\n",
            "Error with GunPointAgeSpan: [Errno 2] No such file or directory: 'processed_datasets/GunPointAgeSpan/test.csv'\n",
            "Error with GunPoint: [Errno 2] No such file or directory: 'processed_datasets/GunPoint/test.csv'\n",
            "Error with GestureMidAirD3: [Errno 2] No such file or directory: 'processed_datasets/GestureMidAirD3/test.csv'\n",
            "Error with GestureMidAirD2: [Errno 2] No such file or directory: 'processed_datasets/GestureMidAirD2/test.csv'\n",
            "Error with GestureMidAirD1: [Errno 2] No such file or directory: 'processed_datasets/GestureMidAirD1/test.csv'\n",
            "Error with Fungi: [Errno 2] No such file or directory: 'processed_datasets/Fungi/test.csv'\n",
            "Error with FreezerSmallTrain: [Errno 2] No such file or directory: 'processed_datasets/FreezerSmallTrain/test.csv'\n",
            "Error with FreezerRegularTrain: [Errno 2] No such file or directory: 'processed_datasets/FreezerRegularTrain/test.csv'\n",
            "Error with FordB: [Errno 2] No such file or directory: 'processed_datasets/FordB/test.csv'\n",
            "Error with FordA: [Errno 2] No such file or directory: 'processed_datasets/FordA/test.csv'\n",
            "Error with Fish: [Errno 2] No such file or directory: 'processed_datasets/Fish/test.csv'\n",
            "Error with FiftyWords: [Errno 2] No such file or directory: 'processed_datasets/FiftyWords/test.csv'\n",
            "Error with FacesUCR: [Errno 2] No such file or directory: 'processed_datasets/FacesUCR/test.csv'\n",
            "Error with FaceFour: [Errno 2] No such file or directory: 'processed_datasets/FaceFour/test.csv'\n",
            "Error with FaceAll: [Errno 2] No such file or directory: 'processed_datasets/FaceAll/test.csv'\n",
            "Error with EthanolLevel: [Errno 2] No such file or directory: 'processed_datasets/EthanolLevel/test.csv'\n",
            "Error with EOGVerticalSignal: [Errno 2] No such file or directory: 'processed_datasets/EOGVerticalSignal/test.csv'\n",
            "Error with EOGHorizontalSignal: [Errno 2] No such file or directory: 'processed_datasets/EOGHorizontalSignal/test.csv'\n",
            "Error with ElectricDevices: [Errno 2] No such file or directory: 'processed_datasets/ElectricDevices/test_target.csv'\n",
            "Error with ECGFiveDays: [Errno 2] No such file or directory: 'processed_datasets/ECGFiveDays/test.csv'\n",
            "Error with ECG5000: [Errno 2] No such file or directory: 'processed_datasets/ECG5000/test_target.csv'\n",
            "Error with ECG200: [Errno 2] No such file or directory: 'processed_datasets/ECG200/test.csv'\n",
            "Error with Earthquakes: [Errno 2] No such file or directory: 'processed_datasets/Earthquakes/test.csv'\n",
            "Error with DodgerLoopWeekend: [Errno 2] No such file or directory: 'processed_datasets/DodgerLoopWeekend/test.csv'\n",
            "Error with DodgerLoopGame: [Errno 2] No such file or directory: 'processed_datasets/DodgerLoopGame/test.csv'\n",
            "Error with DodgerLoopDay: [Errno 2] No such file or directory: 'processed_datasets/DodgerLoopDay/test.csv'\n",
            "Error with DistalPhalanxTW: [Errno 2] No such file or directory: 'processed_datasets/DistalPhalanxTW/test.csv'\n",
            "Error with DistalPhalanxOutlineCorrect: [Errno 2] No such file or directory: 'processed_datasets/DistalPhalanxOutlineCorrect/test.csv'\n",
            "Error with DistalPhalanxOutlineAgeGroup: [Errno 2] No such file or directory: 'processed_datasets/DistalPhalanxOutlineAgeGroup/test.csv'\n",
            "Error with DiatomSizeReduction: [Errno 2] No such file or directory: 'processed_datasets/DiatomSizeReduction/test.csv'\n",
            "Error with Crop: [Errno 2] No such file or directory: 'processed_datasets/Crop/test_target.csv'\n",
            "Error with CricketZ: [Errno 2] No such file or directory: 'processed_datasets/CricketZ/test.csv'\n",
            "Error with CricketY: [Errno 2] No such file or directory: 'processed_datasets/CricketY/test.csv'\n",
            "Error with CricketX: [Errno 2] No such file or directory: 'processed_datasets/CricketX/test.csv'\n",
            "Error with Computers: [Errno 2] No such file or directory: 'processed_datasets/Computers/test_target.csv'\n",
            "Error with Coffee: [Errno 2] No such file or directory: 'processed_datasets/Coffee/test.csv'\n",
            "Error with CinCECGTorso: [Errno 2] No such file or directory: 'processed_datasets/CinCECGTorso/test_target.csv'\n",
            "Error with ChlorineConcentration: [Errno 2] No such file or directory: 'processed_datasets/ChlorineConcentration/test_target.csv'\n",
            "Error with Chinatown: [Errno 2] No such file or directory: 'processed_datasets/Chinatown/test.csv'\n",
            "Error with CBF: [Errno 2] No such file or directory: 'processed_datasets/CBF/test.csv'\n",
            "Error with Car: [Errno 2] No such file or directory: 'processed_datasets/Car/test.csv'\n",
            "Error with BME: [Errno 2] No such file or directory: 'processed_datasets/BME/test.csv'\n",
            "Error with BirdChicken: [Errno 2] No such file or directory: 'processed_datasets/BirdChicken/test.csv'\n",
            "Error with BeetleFly: [Errno 2] No such file or directory: 'processed_datasets/BeetleFly/test.csv'\n",
            "Error with Beef: [Errno 2] No such file or directory: 'processed_datasets/Beef/test_target.csv'\n",
            "Error with Adiac: [Errno 2] No such file or directory: 'processed_datasets/Adiac/test.csv'\n",
            "Error with Yoga: [Errno 2] No such file or directory: 'processed_datasets/Yoga/test.csv'\n",
            "Error with WormsTwoClass: [Errno 2] No such file or directory: 'processed_datasets/WormsTwoClass/test.csv'\n",
            "Error with Worms: [Errno 2] No such file or directory: 'processed_datasets/Worms/test.csv'\n",
            "Error with WordSynonyms: [Errno 2] No such file or directory: 'processed_datasets/WordSynonyms/test.csv'\n",
            "Error with Wine: [Errno 2] No such file or directory: 'processed_datasets/Wine/test.csv'\n",
            "Error with Wafer: [Errno 2] No such file or directory: 'processed_datasets/Wafer/test.csv'\n",
            "Error with UWaveGestureLibraryZ: [Errno 2] No such file or directory: 'processed_datasets/UWaveGestureLibraryZ/test.csv'\n",
            "Error with UWaveGestureLibraryY: [Errno 2] No such file or directory: 'processed_datasets/UWaveGestureLibraryY/test.csv'\n",
            "Error with UWaveGestureLibraryX: [Errno 2] No such file or directory: 'processed_datasets/UWaveGestureLibraryX/test.csv'\n",
            "Error with UWaveGestureLibraryAll: [Errno 2] No such file or directory: 'processed_datasets/UWaveGestureLibraryAll/test.csv'\n",
            "Error with UMD: [Errno 2] No such file or directory: 'processed_datasets/UMD/test.csv'\n",
            "Error with TwoPatterns: [Errno 2] No such file or directory: 'processed_datasets/TwoPatterns/test.csv'\n",
            "Error with TwoLeadECG: [Errno 2] No such file or directory: 'processed_datasets/TwoLeadECG/test.csv'\n",
            "Error with Trace: [Errno 2] No such file or directory: 'processed_datasets/Trace/test.csv'\n",
            "Error with ToeSegmentation2: [Errno 2] No such file or directory: 'processed_datasets/ToeSegmentation2/test.csv'\n",
            "Error with ToeSegmentation1: [Errno 2] No such file or directory: 'processed_datasets/ToeSegmentation1/test.csv'\n",
            "Error with SyntheticControl: [Errno 2] No such file or directory: 'processed_datasets/SyntheticControl/test.csv'\n",
            "Error with Symbols: [Errno 2] No such file or directory: 'processed_datasets/Symbols/test.csv'\n",
            "Error with SwedishLeaf: [Errno 2] No such file or directory: 'processed_datasets/SwedishLeaf/test.csv'\n",
            "Error with Strawberry: [Errno 2] No such file or directory: 'processed_datasets/Strawberry/test.csv'\n",
            "Error with StarLightCurves: [Errno 2] No such file or directory: 'processed_datasets/StarLightCurves/test.csv'\n",
            "Error with SonyAIBORobotSurface2: [Errno 2] No such file or directory: 'processed_datasets/SonyAIBORobotSurface2/test.csv'\n",
            "Error with SonyAIBORobotSurface1: [Errno 2] No such file or directory: 'processed_datasets/SonyAIBORobotSurface1/test.csv'\n",
            "Error with SmoothSubspace: [Errno 2] No such file or directory: 'processed_datasets/SmoothSubspace/test.csv'\n",
            "Error with SmallKitchenAppliances: [Errno 2] No such file or directory: 'processed_datasets/SmallKitchenAppliances/test.csv'\n",
            "Error with ShapesAll: [Errno 2] No such file or directory: 'processed_datasets/ShapesAll/test.csv'\n",
            "Error with ShapeletSim: [Errno 2] No such file or directory: 'processed_datasets/ShapeletSim/test.csv'\n",
            "Error with SemgHandSubjectCh2: [Errno 2] No such file or directory: 'processed_datasets/SemgHandSubjectCh2/test.csv'\n",
            "Error with SemgHandMovementCh2: [Errno 2] No such file or directory: 'processed_datasets/SemgHandMovementCh2/test.csv'\n",
            "Error with SemgHandGenderCh2: [Errno 2] No such file or directory: 'processed_datasets/SemgHandGenderCh2/test.csv'\n",
            "Error with ScreenType: [Errno 2] No such file or directory: 'processed_datasets/ScreenType/test.csv'\n",
            "Error with Rock: [Errno 2] No such file or directory: 'processed_datasets/Rock/test.csv'\n",
            "Error with RefrigerationDevices: [Errno 2] No such file or directory: 'processed_datasets/RefrigerationDevices/test.csv'\n",
            "Error with ProximalPhalanxTW: [Errno 2] No such file or directory: 'processed_datasets/ProximalPhalanxTW/test.csv'\n",
            "Error with ProximalPhalanxOutlineCorrect: [Errno 2] No such file or directory: 'processed_datasets/ProximalPhalanxOutlineCorrect/test.csv'\n",
            "Error with ProximalPhalanxOutlineAgeGroup: [Errno 2] No such file or directory: 'processed_datasets/ProximalPhalanxOutlineAgeGroup/test.csv'\n",
            "Error with PowerCons: [Errno 2] No such file or directory: 'processed_datasets/PowerCons/test.csv'\n",
            "Error with Plane: [Errno 2] No such file or directory: 'processed_datasets/Plane/test.csv'\n",
            "Error with PigCVP: [Errno 2] No such file or directory: 'processed_datasets/PigCVP/test.csv'\n",
            "Error with PigArtPressure: [Errno 2] No such file or directory: 'processed_datasets/PigArtPressure/test.csv'\n",
            "Error with PigAirwayPressure: [Errno 2] No such file or directory: 'processed_datasets/PigAirwayPressure/test.csv'\n",
            "Error with Phoneme: [Errno 2] No such file or directory: 'processed_datasets/Phoneme/test.csv'\n",
            "Error with PhalangesOutlinesCorrect: [Errno 2] No such file or directory: 'processed_datasets/PhalangesOutlinesCorrect/test.csv'\n",
            "Error with OSULeaf: [Errno 2] No such file or directory: 'processed_datasets/OSULeaf/test.csv'\n",
            "Error with OliveOil: [Errno 2] No such file or directory: 'processed_datasets/OliveOil/test.csv'\n",
            "Error with NonInvasiveFetalECGThorax2: [Errno 2] No such file or directory: 'processed_datasets/NonInvasiveFetalECGThorax2/test.csv'\n",
            "Error with NonInvasiveFetalECGThorax1: [Errno 2] No such file or directory: 'processed_datasets/NonInvasiveFetalECGThorax1/test.csv'\n",
            "Error with MoteStrain: [Errno 2] No such file or directory: 'processed_datasets/MoteStrain/test.csv'\n",
            "Error with MixedShapesSmallTrain: [Errno 2] No such file or directory: 'processed_datasets/MixedShapesSmallTrain/test.csv'\n",
            "Error with MixedShapesRegularTrain: [Errno 2] No such file or directory: 'processed_datasets/MixedShapesRegularTrain/test.csv'\n",
            "Error with MiddlePhalanxTW: [Errno 2] No such file or directory: 'processed_datasets/MiddlePhalanxTW/test.csv'\n",
            "Error with MiddlePhalanxOutlineCorrect: [Errno 2] No such file or directory: 'processed_datasets/MiddlePhalanxOutlineCorrect/test.csv'\n",
            "Error with MiddlePhalanxOutlineAgeGroup: [Errno 2] No such file or directory: 'processed_datasets/MiddlePhalanxOutlineAgeGroup/test.csv'\n",
            "Error with MelbournePedestrian: [Errno 2] No such file or directory: 'processed_datasets/MelbournePedestrian/test.csv'\n",
            "Error with MedicalImages: [Errno 2] No such file or directory: 'processed_datasets/MedicalImages/test.csv'\n",
            "Error with Meat: [Errno 2] No such file or directory: 'processed_datasets/Meat/test.csv'\n",
            "Error with Mallat: [Errno 2] No such file or directory: 'processed_datasets/Mallat/test.csv'\n",
            "Error with Lightning7: [Errno 2] No such file or directory: 'processed_datasets/Lightning7/test.csv'\n",
            "Error with Lightning2: [Errno 2] No such file or directory: 'processed_datasets/Lightning2/test.csv'\n",
            "Error with LargeKitchenAppliances: [Errno 2] No such file or directory: 'processed_datasets/LargeKitchenAppliances/test.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "from keras import layers, models, callbacks, regularizers, optimizers\n",
        "\n",
        "# from keras.layers import advanced_activations\n",
        "from contextlib import redirect_stdout\n",
        "import absl.logging\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "\n",
        "def create_CNN_architecture(\n",
        "    window_size,\n",
        "    number_of_layers_in_encoder,\n",
        "    encoder_filters,\n",
        "    activation_functions,\n",
        "    kernel_sizes,\n",
        "    batch_normalizations,\n",
        "    max_poolings,\n",
        "    max_pooling_size=2,\n",
        "    allowed_bottleneck_sizes=[16, 24, 32],\n",
        "    **kwargs,\n",
        "):\n",
        "    TIMESTEPS = window_size\n",
        "    num_inputs = 1\n",
        "    input_placeholder = layers.Input(shape=[TIMESTEPS, num_inputs])\n",
        "    encoded = input_placeholder\n",
        "    for i in range(number_of_layers_in_encoder):\n",
        "        encoder_filter = encoder_filters[i]\n",
        "        activation_function = activation_functions[i]\n",
        "        kernel_size = kernel_sizes[i]\n",
        "        batch_normalization = batch_normalizations[i]\n",
        "        max_pooling = max_poolings[i]\n",
        "\n",
        "        encoded = layers.Conv1D(\n",
        "            encoder_filter,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=\"same\",\n",
        "            activation=activation_function,\n",
        "        )(encoded)\n",
        "        if max_pooling:\n",
        "            encoded = layers.MaxPool1D(max_pooling_size)(encoded)\n",
        "        if batch_normalization:\n",
        "            encoded = layers.BatchNormalization()(encoded)\n",
        "    # bottleneck\n",
        "    encoded = layers.Dense(1, activation=\"relu\")(encoded)\n",
        "    encoded = layers.BatchNormalization(name=f\"embedding\")(encoded)\n",
        "    bottleneck_shape = list(encoded.shape)[1]\n",
        "    # print(f'Bottleneck size: {bottleneck_shape}')\n",
        "    if not (bottleneck_shape in allowed_bottleneck_sizes):\n",
        "        raise Exception(f\"Wrong bottleneck shape: {bottleneck_shape}\")\n",
        "\n",
        "    decoded = encoded\n",
        "\n",
        "    for i in reversed(range(number_of_layers_in_encoder)):\n",
        "        encoder_filter = encoder_filters[i]\n",
        "        activation_function = activation_functions[i]\n",
        "        kernel_size = kernel_sizes[i]\n",
        "        batch_normalization = batch_normalizations[i]\n",
        "        decoded = layers.Conv1DTranspose(\n",
        "            encoder_filter,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=\"same\",\n",
        "            activation=activation_function,\n",
        "        )(decoded)\n",
        "        if max_pooling:\n",
        "            decoded = layers.UpSampling1D(max_pooling_size)(decoded)\n",
        "        if batch_normalization:\n",
        "            decoded = layers.BatchNormalization()(decoded)\n",
        "\n",
        "    decoded = layers.Conv1DTranspose(\n",
        "        filters=1, kernel_size=kernel_size, padding=\"same\"\n",
        "    )(decoded)\n",
        "\n",
        "    autoencoder = models.Model(inputs=input_placeholder, outputs=decoded)\n",
        "    return autoencoder, bottleneck_shape\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_data(main_data_folder, exclude_dataset_for_testing):\n",
        "    data_folders = os.listdir(main_data_folder)\n",
        "    train_data_df = pd.DataFrame()\n",
        "    test_data_df = pd.DataFrame()\n",
        "    exceptions = {}\n",
        "    train_length = 0\n",
        "    test_length = 0\n",
        "    print(f\"Total datasets {len(data_folders)}\")\n",
        "    for f in data_folders:\n",
        "        try:\n",
        "            test_df = pd.read_csv(f\"{main_data_folder}/{f}/test.csv\")\n",
        "            \n",
        "            if f == exclude_dataset_for_testing:\n",
        "                continue\n",
        "            else:\n",
        "                test_length += len(test_df)\n",
        "                train_df = pd.read_csv(f\"{main_data_folder}/{f}/train.csv\")\n",
        "            train_length += len(train_df)\n",
        "            train_data_df = pd.concat(\n",
        "                [train_data_df, train_df], ignore_index=True\n",
        "            )  # train_data_df.append(train_df, ignore_index=True)\n",
        "            test_data_df = pd.concat(\n",
        "                [test_data_df, test_df], ignore_index=True\n",
        "            )  # test_data_df.append(test_df, ignore_index=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            exceptions[f] = e\n",
        "    assert train_length == len(\n",
        "        train_data_df\n",
        "    ), \"Not all training data was appended to final training set\"\n",
        "    assert test_length == len(\n",
        "        test_data_df\n",
        "    ), \"Not all testing data was appended to final testing set\"\n",
        "    return train_data_df, test_data_df, exceptions\n",
        "\n",
        "def compile_model(model, optimizer, loss=\"mse\"):\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    model_name,\n",
        "    train_data,\n",
        "    test_data,\n",
        "    main_model_folder,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "):\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(test_data, test_data),\n",
        "        callbacks=[\n",
        "            callbacks.ModelCheckpoint(\n",
        "                f\"{main_model_folder}/\"\n",
        "                + model_name\n",
        "                + f\"/callbacks\"\n",
        "                + \"/epoch{epoch:02d}-loss{val_loss:.3f}.tf\"\n",
        "            ),\n",
        "            callbacks.ModelCheckpoint(\n",
        "                f\"{main_model_folder}/\" + model_name + f\"/callbacks\" + \"/best.tf\",\n",
        "                save_best_only=True,\n",
        "            ),\n",
        "            callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=5, verbose=1)\n",
        "        ],\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def save_model_data(model, history, main_model_folder):\n",
        "    def save_model_summary(model, path_to_save):\n",
        "        with open(f\"{path_to_save}/model_summary.txt\", \"w\") as f:\n",
        "            with redirect_stdout(f):\n",
        "                model.summary()\n",
        "        pd.DataFrame.from_dict(history.history).to_csv(f\"{path_to_save}/history.csv\")\n",
        "\n",
        "    if not os.path.exists(main_model_folder):\n",
        "        os.mkdir(main_model_folder)\n",
        "    with open(f'{main_model_folder}' + '/model_structure.json', mode='w') as ofile:\n",
        "        ofile.write(model.to_json())\n",
        "    save_model_summary(model, main_model_folder)\n"
      ],
      "metadata": {
        "id": "PoZwvhEXCgrZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "yBvCm5adCCHb",
        "outputId": "0ec2f488-4710-4fad-d5e9-80ce50908d0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_7 = {\\n    \"name\": \"model_8\",\\n    \"window_size\": 128,\\n    \"number_of_layers_in_encoder\": 4,\\n    \"input\": 128,\\n    \"encoder_filters\": [512, 256, 128],\\n    \"kernel_sizes\": [23, 13, 3],\\n    \"activation_functions\": [\"relu\"] * 3,\\n    \"batch_normalizations\": [False] * 3,\\n    \"max_poolings\": [True] * 3,\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "from keras import layers, models, callbacks, regularizers, optimizers\n",
        "#from keras.layers import advanced_activations\n",
        "\n",
        "\n",
        "# Create graph structure.\n",
        "TIMESTEPS = 128\n",
        "num_inputs = 1\n",
        "input_placeholder = layers.Input(shape=[TIMESTEPS, num_inputs])\n",
        "# Encoder.\n",
        "encoded = layers.Conv1D(512, kernel_size=23, padding=\"same\", activation='relu')(input_placeholder)\n",
        "encoded = layers.MaxPool1D(2)(encoded)\n",
        "encoded = layers.BatchNormalization()(encoded)\n",
        "encoded = layers.Conv1D(\n",
        "            filters=256, kernel_size=13, padding=\"same\", activation='relu')(encoded)\n",
        "#encoded = layers.MaxPool1D(2)(encoded)\n",
        "encoded = layers.MaxPool1D(2)(encoded)\n",
        "encoded = layers.BatchNormalization()(encoded)\n",
        "encoded = layers.Conv1D(\n",
        "            filters=256, kernel_size=7, padding=\"same\", activation='relu')(encoded)\n",
        "encoded = layers.MaxPool1D(2)(encoded)\n",
        "encoded = layers.BatchNormalization()(encoded)\n",
        "encoded = layers.Conv1D(\n",
        "            filters=128, kernel_size=3, padding=\"same\", activation='relu')(encoded)\n",
        "encoded = layers.Dense(1, activation='relu')(encoded)\n",
        "encoded = layers.BatchNormalization(name='embedding')(encoded)\n",
        "# Decoder.\n",
        "#decoded = layers.UpSampling1D(2)(encoded)\n",
        "decoded = layers.Conv1DTranspose(128, kernel_size=3, padding=\"same\", activation='relu')(encoded)\n",
        "decoded = layers.Conv1DTranspose(256, kernel_size=7, padding=\"same\", activation=\"relu\")(decoded)\n",
        "decoded = layers.BatchNormalization()(decoded)\n",
        "#decoded = layers.Conv1DTranspose(256, kernel_size=7, padding=\"same\", activation=\"relu\")(decoded)\n",
        "decoded = layers.UpSampling1D(2)(decoded)\n",
        "decoded = layers.Conv1DTranspose(256, kernel_size=7, padding=\"same\", activation=\"relu\")(decoded)\n",
        "decoded = layers.BatchNormalization()(decoded)\n",
        "#decoded = layers.UpSampling1D(2)(decoder)\n",
        "#decoded = layers.BatchNormalization()(decoded)\n",
        "decoded = layers.UpSampling1D(2)(decoded)\n",
        "decoded = layers.Conv1DTranspose(512, kernel_size=13, padding=\"same\", activation=\"relu\")(decoded)\n",
        "#decoded = layers.UpSampling1D(2)(decoder)\n",
        "decoded = layers.BatchNormalization()(decoded)\n",
        "#decoded = layers.Conv1DTranspose(512, kernel_size=23, padding=\"same\", activation=\"relu\")(decoded)\n",
        "decoded = layers.UpSampling1D(2)(decoded)\n",
        "#decoded = layers.UpSampling1D(2)(decoded)\n",
        "#decoded = layers.BatchNormalization()(decoded)\n",
        "#decoded = layers.Conv1DTranspose(2, kernel_size=5, padding=\"same\", strides=2, activation=\"relu\")(decoded)\n",
        "decoded = layers.Conv1DTranspose(filters=1, kernel_size=13, padding=\"same\")(decoded)\n",
        "\n",
        "encoder = models.Model(inputs=input_placeholder, outputs=encoded)\n",
        "model = models.Model(inputs=input_placeholder, outputs=decoded)\n",
        "\n",
        "\"\"\"model_7 = {\n",
        "    \"name\": \"model_8\",\n",
        "    \"window_size\": 128,\n",
        "    \"number_of_layers_in_encoder\": 4,\n",
        "    \"input\": 128,\n",
        "    \"encoder_filters\": [512, 256, 128],\n",
        "    \"kernel_sizes\": [23, 13, 3],\n",
        "    \"activation_functions\": [\"relu\"] * 3,\n",
        "    \"batch_normalizations\": [False] * 3,\n",
        "    \"max_poolings\": [True] * 3,\n",
        "}\"\"\"\n",
        "\n",
        "#model, emb = create_CNN_architecture(**model_7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evmGs4-QCpyu",
        "outputId": "d03ae655-61a6-4825-8a34-29052533f0c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 1)]          0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 128, 512)          12288     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 64, 512)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 64, 512)          2048      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 64, 256)           1704192   \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 32, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 256)          1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 32, 256)           459008    \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 16, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16, 256)          1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 16, 128)           98432     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16, 1)             129       \n",
            "                                                                 \n",
            " embedding (BatchNormalizati  (None, 16, 1)            4         \n",
            " on)                                                             \n",
            "                                                                 \n",
            " conv1d_transpose (Conv1DTra  (None, 16, 128)          512       \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " conv1d_transpose_1 (Conv1DT  (None, 16, 256)          229632    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16, 256)          1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " up_sampling1d (UpSampling1D  (None, 32, 256)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_transpose_2 (Conv1DT  (None, 32, 256)          459008    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 32, 256)          1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " up_sampling1d_1 (UpSampling  (None, 64, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_transpose_3 (Conv1DT  (None, 64, 512)          1704448   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 64, 512)          2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " up_sampling1d_2 (UpSampling  (None, 128, 512)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_transpose_4 (Conv1DT  (None, 128, 1)           6657      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,682,502\n",
            "Trainable params: 4,678,404\n",
            "Non-trainable params: 4,098\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_model_folder = 'trained_models'\n",
        "#os.mkdir(f'{main_model_folder}/')\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "PiTmIJ5BCx-L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exclude_dataset_for_testing = \"InsectSound\"\n",
        "folder_name = \"fully_processed_data/w_128_o_64_p_0\"\n",
        "train_data_df, test_data_df, exceptions = load_data(\n",
        "    folder_name, exclude_dataset_for_testing\n",
        ")\n",
        "train_data = train_data_df.values\n",
        "train_data = train_data.reshape(train_data.shape[0], train_data.shape[1], 1)\n",
        "test_data = test_data_df.values\n",
        "test_data = test_data.reshape(test_data.shape[0], test_data.shape[1], 1)\n",
        "main_model_folder = 'trained_models'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALOLw1AvDE4Z",
        "outputId": "da39cfe9-6a63-42db-fb81-d10c0091446e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total datasets 97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kccU81Uwz7Hk",
        "outputId": "f20d85a7-9f99-44cb-b079-5fa105fec969"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(291270, 128, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRLKHq_bz93F",
        "outputId": "5537dc21-0feb-46fa-cac7-136f757fe57c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(190556, 128, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "291270 + 190556"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InlYnk8y0J_P",
        "outputId": "b125e153-9e83-4cf8-9152-c04f21d68e52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "481826"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "481826 * 0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c5qcHtA0MQb",
        "outputId": "25d50299-119f-4289-e759-a6c80981da62"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96365.20000000001"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_arch = 'model_12'\n",
        "model, embed = model, 32#create_CNN_architecture(**model_arch)\n",
        "opt = optimizers.Adam(learning_rate=.0001)\n",
        "epochs = 100\n",
        "k = 'model_12'\n",
        "embedding_size = embed\n",
        "model_arch = None\n",
        "print(f'Model iteration: 0 name: {k}')\n",
        "if model_arch is not None:\n",
        "  number_of_layers_in_encoder = model_arch['number_of_layers_in_encoder']\n",
        "  encoder_filters = model_arch['encoder_filters']\n",
        "  kernel_sizes = model_arch['kernel_sizes']\n",
        "  print(f'INFO: Layers: {number_of_layers_in_encoder} | embedding size {embedding_size} | Kernel filters {encoder_filters} | Kernel sizes {kernel_sizes}')\n",
        "model.compile(optimizer=opt, loss='mse')\n",
        "folder_name = f'{main_model_folder}/{k}'\n",
        "history = train_model(model, k, train_data, test_data, main_model_folder, epochs=epochs)\n",
        "save_model_data(model, history, folder_name)\n",
        "re = model.evaluate(test_data)\n",
        "hist_df = pd.DataFrame.from_dict(history.history)\n",
        "lowest_test_val_loss = hist_df.iloc[hist_df['val_loss'].argmin()]\n",
        "print(f'Model {k} results {re}')\n",
        "vals = lowest_test_val_loss.values\n",
        "print(f'Val loss: train {vals[0]} test: {vals[1]}')\n",
        "print(\"========================= Finished training model =========================\")\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "8SvT2sZ-CqI2",
        "outputId": "23d5ef59-7dd6-4b66-afd2-77c5560ac7bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model iteration: 0 name: model_12\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-10aa8ff1adbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{main_model_folder}/{k}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_model_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0msave_model_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-76dd8645add4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, model_name, train_data, test_data, main_model_folder, epochs, batch_size)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m ):\n\u001b[0;32m--> 133\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHbcSyqtHZ74"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}